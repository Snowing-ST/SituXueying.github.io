I"½<font face="ä»¿å®‹">åŸºäºæ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»çš„ç½‘ç»œæ–°é—»æƒ…æ„ŸæŒ‡æ•°ç¼–åˆ¶ï¼ˆå››ï¼‰<br />æ–‡æœ¬åˆ†ç±»åŸºå‡†æ¨¡å‹ï¼šå‘é‡ç©ºé—´æ¨¡å‹</font>
<style>
    body {font-family: "åæ–‡ä¸­å®‹"}
</style>

<h2 id="main-step-4traditional-text-classification-with-vsm">Main step 4:<center>traditional text classification with VSM</center></h2>
<h3 id="description">description:</h3>
<ul>
  <li>text representation: TF-IDF</li>
  <li>classification model ï¼šlogisticã€NaÃ¯ve Bayesã€SVM</li>
  <li>best model ï¼šSVM, accuracy:77% (as basic line)</li>
</ul>

<h3 id="code-explanation">code explanation:</h3>

<h4 id="1-word-representation">1. word representation</h4>
<ul>
  <li>two approaches to transform text into matrix:
    <ol>
      <li>tf-idf 2. one-hot</li>
    </ol>
  </li>
  <li>input:two columns name <code class="highlighter-rouge">word_seg</code> and <code class="highlighter-rouge">label</code> of  <code class="highlighter-rouge">data_train</code> and <code class="highlighter-rouge">data_test</code></li>
  <li>output: <code class="highlighter-rouge">data_train_tfidf</code>, <code class="highlighter-rouge">tags</code> ,<code class="highlighter-rouge">data_test_tfidf</code>
```python
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer</li>
</ul>

<p>def vectorize(data_train,data_test,word_name = â€œword_segâ€,tag_name=â€labelâ€,vectype = â€œtf-idfâ€,ngram_range=(0,1),max_features=None,min_df=2):
    â€œâ€â€
    æ–‡æœ¬è¡¨ç¤ºï¼štf-idf or one-hot
    â€œâ€â€</p>
<h1 id="dataisnullanyå“ªäº›åˆ—å­˜åœ¨ç¼ºå¤±å€¼">data.isnull().any()#å“ªäº›åˆ—å­˜åœ¨ç¼ºå¤±å€¼</h1>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>words = data_train[word_name].tolist()+data_test[word_name].tolist()
if vectype == "tf-idf":
    transformer=TfidfVectorizer(ngram_range=ngram_range,max_features=max_features,min_df=min_df)
    data_tfidf=transformer.fit_transform(words)
    transformer2 = TfidfVectorizer(vocabulary = transformer.vocabulary_)
    data_train_tfidf=transformer2.fit_transform(data_train[word_name])
    data_test_tfidf=transformer2.fit_transform(data_test[word_name])          
    return data_train_tfidf,data_train[tag_name].tolist(),data_test_tfidf
if vectype == "one-hot":
    transformer=CountVectorizer(ngram_range=ngram_range,max_features=max_features,min_df=min_df)
     
    data_onehot=transformer.fit_transform(words)
    transformer2 = CountVectorizer(vocabulary = transformer.vocabulary_)
    data_train_onehot=transformer2.fit_transform(data_train[word_name])
    data_test_onehot=transformer2.fit_transform(data_test[word_name])           
    return data_train_onehot,data_train[tag_name].tolist(),data_test_onehot
</code></pre></div></div>

<p>data_train_tfidf, tags ,data_test_tfidf = vectorize(data_train,data_test,word_name = â€œword_segâ€,tag_name=â€labelâ€,vectype = â€œtf-idfâ€,ngram_range=(0,1),max_features=None,min_df=1)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
#### 2. several machine learning approaches
1. naive bayes 2. logistic regression 3. SVM
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression  
from sklearn import svm
from sklearn.model_selection import GridSearchCV,cross_val_score, cross_val_predict,KFold

def train_NB(data_tfidf,tags,cv):
    grid_values = {'alpha':np.arange(0.1,1.1,0.1)} # Decide which settings you want for the grid search. 

    grid = GridSearchCV(MultinomialNB(), 
                        grid_values, scoring = "accuracy", cv = cv) 
    grid.fit(data_tfidf,tags) 
    grid.grid_scores_
    print("ã€NBã€‘The best parameters are %s with a score of %0.4f"
          % (grid.best_params_, grid.best_score_))
    return grid.best_estimator_

def train_lg(data_tfidf,tags,cv):
    grid_values = {'tol':[0.001,0.1,1],'C':range(1,10,2)} # Decide which settings you want for the grid search. 

    grid = GridSearchCV(LogisticRegression(penalty="l2", dual=True), 
                        grid_values, scoring = "accuracy", cv = cv,n_jobs=7) 
    grid.fit(data_tfidf,tags) 
    grid.grid_scores_
    print("ã€lgã€‘The best parameters are %s with a score of %0.4f"
          % (grid.best_params_, grid.best_score_))
    return grid.best_estimator_

def train_SVM(data_tfidf,tags,cv):#5,1,1 #4,0.9,1 0.8238
    "è°ƒå‚å½±å“å¤§ã€‚å­¦ä¹ ç‡è¶Šå°ï¼Œæ‰€éœ€è¿­ä»£æ¬¡æ•°è¶Šå¤š"
    grid_values = {'C':[1,4,7],'gamma':[0.1,0.5,0.9]} # Decide which settings you want for the grid search. 

    grid = GridSearchCV(svm.SVC(kernel='rbf',tol=1, degree=3, coef0=0.0, shrinking=True, probability=False),
                        grid_values, scoring = "accuracy", cv = cv) 
    grid.fit(data_tfidf,tags) 
    grid.grid_scores_
    print("ã€SVMã€‘The best parameters are %s with a score of %0.4f"
          % (grid.best_params_, grid.best_score_))
    return grid.best_estimator_

#æ¨¡å‹æ¯”è¾ƒ
cv = KFold(n_splits=10, shuffle=True, random_state=1994)
NB = train_NB(data_train_tfidf, tags,cv)
lg = train_lg(data_train_tfidf, tags,cv)
SVM = train_SVM(data_train_tfidf, tags,cv)
</code></pre></div></div>

<p>For more information about this project,please visit my <a href="https://github.com/Snowing-ST/Construction-and-Application-of-Online-News-Sentiment-Index">github</a>.</p>
:ET